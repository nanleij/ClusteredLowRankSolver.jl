var documenterSearchIndex = {"docs":
[{"location":"examples/clustering/#Example:-Clustering","page":"Clustering","title":"Example: Clustering","text":"","category":"section"},{"location":"examples/clustering/","page":"Clustering","title":"Clustering","text":"Here we give a small example in which it is beneficial to use the option as_free to model positive semidefinite variables as free variables. We focus on the constraints. Suppose you want to solve a semidefinite program with the constraint that ","category":"page"},{"location":"examples/clustering/","page":"Clustering","title":"Clustering","text":"beginalign*\n    langle Y A(x) rangle\nendalign*","category":"page"},{"location":"examples/clustering/","page":"Clustering","title":"Clustering","text":"is nonnegative on a union of k semialgebraic sets ","category":"page"},{"location":"examples/clustering/","page":"Clustering","title":"Clustering","text":"beginalign*\n    G_i = x in mathbbR^n  g_j^i(x) geq 0 j=1 ldots m_i\nendalign*","category":"page"},{"location":"examples/clustering/","page":"Clustering","title":"Clustering","text":"and suppose that these semialgebraic sets are archimedean, so that we can use Putinar's theorem. Then this translates into k sum-of-squares constraints; one for each semialgebraic set.","category":"page"},{"location":"examples/clustering/","page":"Clustering","title":"Clustering","text":"Assuming that the low-rank matrix A is defined before, as well as the polynomials g[i][j], the basis sosbasis[i][j] of the correct degrees and the sample points samples, this gives the code","category":"page"},{"location":"examples/clustering/","page":"Clustering","title":"Clustering","text":"    constraints = []\n    for i=1:k\n        psd_dict = Dict()\n        psd_dict[Block(:Y)] = A\n        for j=1:m[i]\n            psd_dict[Block((:sos,i,j))] = LowRankMatPol([-g[i][j]], [sosbasis[i][j]])\n        end\n        push!(constraints, Constraint(0,psd_dict,Dict(), samples))\n    end","category":"page"},{"location":"examples/clustering/","page":"Clustering","title":"Clustering","text":"Since the positive semidefinite matrix variable Y occurs in every constraint, the corresponding cluster contains k cdot S constraints after sampling, where S is the number of samples. To split this into k clusters of S constraints, we use the option as_free to model Y as free variables:","category":"page"},{"location":"examples/clustering/","page":"Clustering","title":"Clustering","text":"    polprob = LowRankPolProblem(false,obj, constraints)\n    sdp = ClusteredLowRankSDP(polprob; as_free = [:Y])","category":"page"},{"location":"examples/clustering/","page":"Clustering","title":"Clustering","text":"This adds auxilliary free variables X_ij, adds the constraints X_ij = Y_ij, and replaces the Y_ij in the constraints by X_ij. Then the only positive semidefinite variables in the polynomial constraints are the sums-of-squares matrices, which causes each sums-of-squares constraint to be assigned to its own cluster.","category":"page"},{"location":"references/#References-and-citing","page":"References and citing","title":"References and citing","text":"","category":"section"},{"location":"references/","page":"References and citing","title":"References and citing","text":"If you use ClusteredLowRankSolver.jl in work that results in a publication, please consider citing","category":"page"},{"location":"references/","page":"References and citing","title":"References and citing","text":"D. de Laat and N.M. Leijenhorst, Solving Clustered Low-Rank Semidefinite Programs arising from Polynomial Optimization, arXiv:2202.12077 [math], 2022","category":"page"},{"location":"references/#References","page":"References and citing","title":"References","text":"","category":"section"},{"location":"references/","page":"References and citing","title":"References and citing","text":"","category":"page"},{"location":"manual/interface/#Interface","page":"Interface","title":"Interface","text":"","category":"section"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"In this section we will explain the interface, which is focused on semidefinite programs with polynomial constraints. The most general form of such a problem is","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"beginaligned\n    min quad  sum_j langle Y^j C^j rangle + langle y brangle \n    textst quad  langle Y^j A^j_*(x) rangle + B^T(x) y = c(x) \n     Y^j succeq 0\nendaligned","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"where we optimize over free scalar variables y and positive semidefinite variables Y^j = mathrmdiag(Y^j1 ldots Y^jL_j). The polynomial matrices A^j(x) are required to be symmetric and have a block form with low-rank and normal blocks. That is, A^j(x) is block-diagonal with blocks A^jl(x) and","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"    A^jl(x) = sum_rs=1^N_jl A^jl_rs(x) otimes E_rs^R_j(l)","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"where E_rs^N is the N times N matrix with a one in the (rs) entry and zeros elsewhere. Furthermore, A^jl_rs(x) can be of low rank such that A^jl_rs = (A^jl_sr)^T. Often, the non-diagonal block structure is not used (N=1 for all jl). One example where it is used is in polynomial matrix programs (see, e.g., David de Laat, Nando Leijenhorst (2022)). ","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"This is then converted to a clustered low-rank semidefinite program by Sampling.","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"We will explain the interface using an example from polynomial optimization. Suppose we have some polynomial, e.g.,","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"f(xyz) = x^4 + y^4 + z^4 - 4xyz + x + y + z","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"and we want to find the minimum of this polynomial, or an upper bound on the minimum. We can relax the problem using a sum-of-squares characterization:","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"beginaligned\n    min quad  M  \n    textst quad  f-M  text is a sum-of-squares \nendaligned","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"Given a vector w(xyz) of basis polynomials up to degree d, we can parametrize a sum-of-squares polynomial s of degree 2d by a positive semidefinite matrix Y with","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"    s =  Y ww^sf T ","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"See the examples section for more complicated examples, explaining more intricate parts of the interface.","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"using ClusteredLowRankSolver, AbstractAlgebra, BasesAndSamples\n\nfunction min_f(d)\n    # Set up the polynomial space and define f\n    R, (x,y,z) = PolynomialRing(RealField, [\"x\", \"y\", \"z\"])\n    f = x^4 + y^4 + z^4 - 4x*y*z + x + y + z\n\n    # Define the objective\n    obj = Objective(0, Dict(), Dict(:M => 1))\n\n    # Define the constraint SOS + M = f\n    # free variables\n    free_dict = Dict(:M => 1)\n\n    # PSD variables (the sum of squares polynomial) & samples\n    psd_dict = Dict()\n    w = basis_monomial(2d, x, y, z)\n    samples = sample_points_simplex(3,2d)\n    basis, samples = approximatefekete(w, samples)\n\n    psd_dict[Block(:Y)] = LowRankMatPol([1], [basis[1:binomial(3+d,d)]])\n\n    # the constraint\n    con  = Constraint(f, psd_dict, free_dict, samples)\n\n    # Define the polynomial program (maximize obj s.t. con)\n    pol_prob = LowRankPolProblem(true, obj, [con])\n\n    # Convert to a clustered low-rank SDP\n    sdp = ClusteredLowRankSDP(pol_prob)\n\n    #solve the SDP\n    status, sol, time, errorcode = solvesdp(sdp)\nend","category":"page"},{"location":"manual/interface/#Objective","page":"Interface","title":"Objective","text":"","category":"section"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"For the Objective, we need the constant, a dictionary with the C^jl matrices, and a dictionary with the entries of b. In this case, the constant is 0, the matrix C is the zero matrix, which can be omitted, and the vector b has one entry corresponding to the free variable M:","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"    # Define the objective\n    obj = Objective(0, Dict(), Dict(:M => 1))","category":"page"},{"location":"manual/interface/#Constraints","page":"Interface","title":"Constraints","text":"","category":"section"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"For a constraint, we need the low-rank matrices A^jl_rs(x), the entries of B^sf T(x) and the polynomial c(x). Furthermore, we require a unisolvent set of sample points. In our case, c = f, the entry of B corresponding to M is the constant polynomial 1, and the low-rank matrix corresponding to our PSD matrix variables is the matrix ww^sf T. With","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"    w = monomial_basis(2d, x, y, z)","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"we use BasesAndSamples to create the monomial basis for w. Then we create a unisolvent set of sample points for polynomials in three variables up to degree 2d by","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"    samples = sample_points_simplex(3,2d)","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"In general, this is not a very good combination. We can improve the basis with approximatefekete, which orthogonalizes the basis with respect to the sample points. If samples would contain more samples than needed, this would also select a good subset of the samples.","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"    basis, samples = approximatefekete(w, samples)","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"returns the basis in evaluated form, which we can only evaluate on samples from samples. Common operations such as multiplications and additions work with these sampled polynomials, but for example extracting the degree is not possible since that requires expressing the polynomials in a graded basis. However, if w is a basis ordered on degree, basis will have the same ordering. To create the LowRankMatPol for the sum-of-squares polynomial, we use","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"    psd_dict[Block(:Y)] = LowRankMatPol([R(1)], [basis[1:binomial(3+d,d)]])","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"Using Block(:Y) as key indicates that we use the (rs) = (11) block of the matrix :Y; Block(:Y, r, s) would use the (r,s) subblock. The size of the matrices is implicitely given by the size of the LowRankMatPol, which is defined by the prefactors and the rank one terms. In this case, we want to use the basis up to degree d to get a sum-of-squares polynomial up to degree 2d.","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"The command","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"    con  = Constraint(f, psd_dict, free_dict, samples)","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"creates the constraint.","category":"page"},{"location":"manual/interface/#Low-rank-polynomial-programs","page":"Interface","title":"Low rank polynomial programs","text":"","category":"section"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"With","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"    pol_prob = LowRankPolProblem(true, obj, [con])","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"we create a polynomial problem (LowRankPolProblem). The first argument indicates whether we maximize (true) or minimize (false) the objective (obj) with respect to the constraints (in this case one constraint con).","category":"page"},{"location":"manual/interface/#Clustered-low-rank-semidefinite-programs","page":"Interface","title":"Clustered low rank semidefinite programs","text":"","category":"section"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"Finally, we convert the polynomial program to a clustered low-rank semidefinite program by sampling with","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"    sdp = ClusteredLowRankSDP(pol_prob)","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"which we can solve with","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"    status, sol, time, errorcode = solvesdp(sdp)","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"This function has multiple options; see the section with solver Options.","category":"page"},{"location":"manual/interface/#Retrieving-variables-from-the-solution","page":"Interface","title":"Retrieving variables from the solution","text":"","category":"section"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"The solver outputs the solution sol in the form of a CLRSResults struct, which may be used to retrieve variables and the objective from the solver. In the following we want to obtain the (dual) objective of a solution, and the value of a free variable named :a.","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"    objective = sol.dual_objective\n    a_value = sol.freevar[:a]","category":"page"},{"location":"manual/interface/","page":"Interface","title":"Interface","text":"Similarly, we can retrieve the matrix variable :Y with sol.matrixvar[:Y].","category":"page"},{"location":"manual/sampling_clustering/#Sampling-and-Clustering","page":"Sampling and Clustering","title":"Sampling and Clustering","text":"","category":"section"},{"location":"manual/sampling_clustering/#Sampling","page":"Sampling and Clustering","title":"Sampling","text":"","category":"section"},{"location":"manual/sampling_clustering/","page":"Sampling and Clustering","title":"Sampling and Clustering","text":"For the samples, we need a minimal unisolvent set of points. A set of points is unisolvent for a space of polynomials if the only polynomial which evaluates to 0 on all the points is the zero polynomial. Such a set is minimal if any strict subset is not unisolvent. In the univariate case, any d+1 distinct points are unisolvent for polynomials up to degree d. In general, it is possible to test whether a set of points is unisolvent by creating the Vandermonde matrix (b_j(x_i))_ij and checking whether it is nonsingular. This matrix is also used by approximatefekete to select a subset of good sample points and a correspondingly good basis. ","category":"page"},{"location":"manual/sampling_clustering/","page":"Sampling and Clustering","title":"Sampling and Clustering","text":"There are two main approaches to create a unisolvent set of points. First, one can take a number of random samples (at least equal to the dimension of the polynomial space considered in the constraint). This is a unisolvent set with probability 1, and by using the function approximatefekete it is possible to select a minimal unisolvent subset of these points, together with a good basis. The second approach is to take a grid of points, and again use approximatefekete to select a minimal unisolvent subset. ","category":"page"},{"location":"manual/sampling_clustering/#Clustering","page":"Sampling and Clustering","title":"Clustering","text":"","category":"section"},{"location":"manual/sampling_clustering/","page":"Sampling and Clustering","title":"Sampling and Clustering","text":"In the ClusteredLowRankSDP, the constraints are clustered. That is, two constraints in different clusters do not use the same positive semidefinite matrix variables. Internally, this creates a block structure which can be exploited by the solver. One example where clustering occurs is when sampling multiple polynomial constraints which do not use positive semidefinite variables other than the ones for sum-of-squares characterizations. If other positive semidefinite variables are used, it might be beneficial to use extra free variables in the constraint instead of the positive semidefinite matrix variables, and add constraint to equate these variables to entries of the positive semidefinite matrix variables. This is supported with the keyword argument as_free:","category":"page"},{"location":"manual/sampling_clustering/","page":"Sampling and Clustering","title":"Sampling and Clustering","text":" \tsdp = ClusteredLowRankSDP(pol_prob, as_free = [:A, :B,  ...])","category":"page"},{"location":"manual/sampling_clustering/","page":"Sampling and Clustering","title":"Sampling and Clustering","text":"where :A, :B, ... are the keys in the Block structure corresponding to the positive semidefinite matrix variables which should be modelled using extra free variables. See Clustering for an example. This option can also be used to avoid high-rank constraint matrices. ","category":"page"},{"location":"manual/solver/#The-solver","page":"The solver","title":"The solver","text":"","category":"section"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"ClusteredLowRankSolver is a primal-dual interior point method. Besides the dual program, it simultaneously solves the primal program","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"beginaligned\n  max  sum_j=1^J langle c^j x^j rangle \n  textst  sum_j=1^J (B^j)^sf T x^j = b \n    X^j= sum_p=1^P_j x_p^j A_p^j - C^j succeq 0quad j=1ldotsJ\nendaligned","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"Starting from a (probably infeasible) solution, the solver iteratively improves the solution using Newton steps until it is sufficiently close to being feasible and optimal. For a discussion of the general algorithm, see David Simmons-Duffin (2015); for the modifications needed to be able to use the general low-rank structures and for a discussion of the parallelization strategy, see David de Laat, Nando Leijenhorst (2022). The feasibility is measured by the violation of the primal and dual constraints given by","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"beginaligned\n    p = sum_j=1^J (B^j)^sf T x^j - b \n    P^j = X^j - sum_p=1^P_j x_p^j A_p^j + C^j \n    d^j = biglangle A_*^j Y^jbigrangle + B^jy - c^j\nendaligned","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"We denote the primal and dual errors by","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"  ε_p = max(max_r( p_r) max_jrs(P^j_rs))","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"and","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"    ε_d = max_jr( d^j_r)","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"respectively. A solution (xXyY) is primal (dual) feasible if the primal (dual) error is at most the threshold primal_error_threshold (dual_error_threshold). A solution is feasible if it is both primal and dual feasible.","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"Denoting the primal objective by O_p(xX) and the dual objective by O_d(yY), a measure of optimality is given by the (normalized) duality gap","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"    Δ = fracO_p(xX) - O_d(yY)max(1O_p(xX) + O_d(yY) )","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"which measures the relative difference between the objectives of the primal and the dual problem. A solution is considered optimal if it is feasible and if the duality gap is at most duality_gap_threshold. The solver iteratively improves the feasibility and the duality gap of the solution.","category":"page"},{"location":"manual/solver/#Options","page":"The solver","title":"Options","text":"","category":"section"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"Here we list the most important options. For the remaining options, see the documentation and the explanation of the algorithm in David Simmons-Duffin (2015).","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"prec - The number of bits used for the calculations. The default is the BigFloat precision, which defaults to 256 bits.\nduality_gap_threshold - Gives an indication of how close the solution is to the optimal solution. As a rule of thumb, a duality gap of 10^-(k+1) gives k correct digits.  Default: 10^-15\ngamma - The step length reduction; if a step of alpha is possible, a step of min(gamma alpha 1) is taken. A lower gamma results in a more stable convergence, but can be significantly slower. Default: 09.\nomega_p, omega_d - The size of the initial primal respectively dual solution. A low omega can keep the solver from converging, but a high omega in general increases the number of iterations needed and thus also the solving time. Default: 10^10\nneed_primal_feasible, need_dual_feasible - If true, terminate when a primal or dual feasible solution is found, respectively. Default: false.","category":"page"},{"location":"manual/solver/#Output","page":"The solver","title":"Output","text":"","category":"section"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"When the option verbose is true (default), the solver will output information for every iteration. In order of output, we have (where the values are from the start of the iteration except for the step lengths, which are only known at the end of the iteration)","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"The iteration number\nThe time since the start of the first iteration\nThe complementary gap mu = langle X Y rangle  K where K is the number of rows of X. The solution will converge to the optimum for mu to 0.\nThe primal objective O_p(yY)\nThe dual objective O_d(yY)\nThe relative duality gap Δ\nThe primal matrix error max_jrsP^j_rs.\nThe primal scalar error max_rp_r\nThe dual (scalar) error ε_d\nThe primal step length\nThe dual step length\nbeta_c. The solver tries to reduce mu by this factor in this iteration.","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"An example of the output of the example used to explain the interface is","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"iter  time(s)           μ       P-obj       D-obj        gap    P-error    p-error    d-error        α_p        α_d       beta\n   1     12.8   1.000e+20   0.000e+00   0.000e+00   0.00e+00   1.00e+10   1.00e+00   6.43e+09   7.51e-01   7.58e-01   3.00e-01\n   2     13.2   3.547e+19   3.624e+10  -9.588e+08   1.05e+00   2.49e+09   2.49e-01   1.56e+09   6.97e-01   7.05e-01   3.00e-01\n...\n  60     13.8   1.216e-14  -2.113e+00  -2.113e+00   2.88e-14   4.53e-76   1.71e-75   8.94e-71   1.00e+00   1.00e+00   1.00e-01\n  61     13.8   1.216e-15  -2.113e+00  -2.113e+00   2.88e-15   5.53e-76   7.65e-75   1.67e-70   1.00e+00   1.00e+00   1.00e-01\nOptimal solution found\n13.806709 seconds (31.85 M allocations: 1.724 GiB, 8.04% gc time, 95.01% compilation time)\niter  time(s)           μ       P-obj       D-obj        gap    P-error    p-error    d-error        α_p        α_d       beta\n\nPrimal objective:[-2.1129138814236035493303617046125433190930634590630701102555586784046703753 +/- 8.99e-74]\nPrimal objective:[-2.1129138814236047658789644991030406911852260781814341016229113808001230237954 +/- 3.25e-77]\nDuality gap:[2.878840953931412734986211198407702873449111279309426433372e-16 +/- 2.66e-74]","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"Note that the first iteration takes long because the functions used by the solver get compiled.","category":"page"},{"location":"manual/solver/#Status","page":"The solver","title":"Status","text":"","category":"section"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"When the algorithm finishes due to one of the termination criteria, the status, the final solution together with the objectives, the used time and an error code is returned. The status can be one of","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"Optimal\nNearOptimal -   The solution is primal and dual feasible, and the duality gap is small (10^-8), although not smaller than duality_gap_threshold.\nFeasible\nPrimalFeasible or DualFeasible\nNotConverged","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"The final solution is stored in the CLRSResults structure. This includes the variables (xXyY), the primal and dual objective, and the self-chosen names corresponding to the variables Y and y.","category":"page"},{"location":"manual/solver/#Errors","page":"The solver","title":"Errors","text":"","category":"section"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"Although unwanted, errors can be part of the output as well. The error codes give an indication what a possible solution could be to avoid the errors.","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"No error\nAn arbitrary error. This can be an internal error such as a decomposition that was unsuccessful. If this occurs in the first iteration, it is a strong indication that the constraints are linearly dependent, e.g. due to using a set of sample points which is not minimal unisolvent for the basis used. Otherwise increasing the precision may help. This also includes errors which are due to external factors such as a keyboard interrupt.\nThe maximum number of iterations has been exceeded. Reasons include: slow convergence, a difficult problem. Possible solutions: increase the maximum number of iterations, increase gamma (if gamma is small), change the starting solution (omega_p and omega_d).\nThe maximum complementary gap (mu) has been exceeded. Usually this indicates (primal and/or dual) infeasibility.\nThe step length is below the step length threshold. This indicates precision errors or a difficult problem. This may be solved by increasing the initial solutions (omega_p and omega_d), or by decreasing the step length reduction gamma, or by increasing the precision prec.","category":"page"},{"location":"manual/solver/#Multithreading","page":"The solver","title":"Multithreading","text":"","category":"section"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"The solver supports multithreading. This can be used by starting julia with","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"julia -t n","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"where n denotes the number of threads.","category":"page"},{"location":"manual/solver/","page":"The solver","title":"The solver","text":"warning: Warning\nOn Windows, using multiple threads can lead to errors when using multiple clusters and free variables. This is probably related to Arb or the Julia interface to Arb.","category":"page"},{"location":"examples/sphere_packing/#Example:-Binary-sphere-packing","page":"Sphere packing","title":"Example: Binary sphere packing","text":"","category":"section"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"This is a slightly more advanced example, which uses the non-diagonal block structure of the constraint matrices. We consider the maximum density which can be obtained by packing spheres with radii r_1 ldots r_N in R^n. In David de Laat, Fernando Mário de Oliveira Filho, Frank Vallentin (2014) the Cohn-Elkies linear programming bound for sphere packing densities Henry Cohn, Noam Elkies (2003) is generalized to N radii. We will follow their approach to define an optimization problem in terms of polynomial equality constraints.","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"compat: Theorem\nLet r_1ldotsr_N0, and let fmathbbR^n to mathbbR^N times N be a matrix-valued function whose every component f_ij is a radial Schwartz function. Suppose f satisfies the following conditions:The matrix big(hatf_ij(0)- (mathrmvol B(r_i))^12(mathrmvol B(r_j))^12big)_ij=1^N is positive semidefinite, where B(r) is the ball of radius r centered at the origin.\nThe matrix of Fourier transforms big(hatf_ij(t)big)_ij=1^N is positive semidefinite for every t0.\nf_ij(w)leq 0 if w geq r_i+r_j, for ij = 1ldotsN.Then the density of any sphere packing of spheres of radii r_1ldotsr_N in the Euclidean space mathbbR^n is at most max f_ii(0) i=1ldotsN","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"Using functions of the form","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"widehatf(x) = sum_k=0^d A_k x^2k e^-pi x^2","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"we obtain by Lemma 5.3 of David de Laat, Fernando Mário de Oliveira Filho, Frank Vallentin (2014)","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"f(x) = sum_k=0^d A_k frackpi^kL_k^n2-1(pi x^2)e^-pi x^2","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"where A_k are real, symmetric N times N matrices, and L_k^n2-1 is the degree k Laguerre polynomial with parameter n2-1. Since positive factors do not influence positivity, we can ignore the factors e^-pi x^2. We denote the resulting polynomials still by f and hatf, abusing notation. This gives the constraint for first requirement of the Theorem","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"hatf(0) - big((mathrmvol B(r_i))^12(mathrmvol B(r_j))^12big)_ij=1^N = Y_0 succeq 0","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"We can apply this constraint element-wise to get N(N-1)2 constraints enforcing the first requirement.","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"Using the theory of matrix polynomial programs (see e.g. David de Laat, Nando Leijenhorst (2022)), we know that the second requirement is fulfilled if and only if there are positive semidefinite matrices Y_1 and Y_2 such that","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"hatf(t)_ij = langle b(t)b(t)^sf T otimes E_ij Y_1 rangle + langle tb(t)b(t)^sf T otimes E_ij Y_2 rangle","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"for every entry hatf_ij, ij=1ldots N.","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"We can enforce the third requirement with the constraints that there are positive semidefinite matrices Z_ij^l for l=12 and ij=1ldotsN such that","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"f(w)_ij + langle b(w)b(w)^sf T Z_ij^1 rangle + langle (w-(r_i+r_j)^2)b(w)b(w)^sf T Z_ij^2 rangle = 0","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"where we denote w = x^2 (note that f(x)_ij is a polynomial in x^2).","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"Finally, the objective can be modelled by introducing a free variable M and requiring that","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"f_ii(0) leq M","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"where we minimize over M. This assures that M = maxf_ii(0), which equals the bound given by the function f.","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"In the following, we will show how to model the second type of constraint. This constraint contains the internal block structure, which is not present in the previous example of the Delsarte linear programming bound. For this, we assume we defined a certain maximum degree d and the dimension n. We use the packages AbstractAlgebra, ClusteredLowRankSolver and BasesAndSamples. As in the previous example, we first construct a suitable basis and sample points.","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"R, (x,) = PolynomialRing(RealField, [\"x\"])\n\nbasis = basis_laguerre(2d+1, BigFloat(n)/2-1, BigFloat(2*pi)*x)\nsamples = sample_points_rescaled_laguerre(2d+1)\nbasis, samples = approximatefekete(basis, samples)","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"Now, for given i != j (over which we normally would loop to get all constraints), we can start defining the corresponding constraint.","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"free_dict = Dict()\nfor k = 0:2d+1\n    # Although we don't need to use the Block structure,\n    # we can of course do it if it is more convenient.\n    # Instead, we could e.g. use (:A, k, i, j) as key\n    free_dict[Block(k, i, j)] = -x^k\nend\n\npsd_dict = Dict()\n# We need to keep the blocks symmetric,\n# so we distribute the matrices over the two blocks.\npsd_dict[Block(:SOS21, i, j)] = LowRankMatPol([R(1//2)], [basis[1:d+1]])\npsd_dict[Block(:SOS21, j, i)] = LowRankMatPol([R(1//2)], [basis[1:d+1]])\n\npsd_dict[Block(:SOS22, i, j)] = LowRankMatPol([1//2*x], [basis[1:d+1]])\npsd_dict[Block(:SOS22, j, i)] = LowRankMatPol([1//2*x], [basis[1:d+1]])\n\nconstr = Constraint(R(0), psd_dict, free_dict, samples)","category":"page"},{"location":"examples/sphere_packing/","page":"Sphere packing","title":"Sphere packing","text":"For i = j, the definition is similar, except that we do not need to distribute the constraint matrices over two blocks. See the Examples folder for the full code.","category":"page"},{"location":"#ClusteredLowRankSolver.jl","page":"Home","title":"ClusteredLowRankSolver.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ClusteredLowRankSolver.jl provides a primal-dual interior point method for solving clustered low-rank semidefinite programs. This can be used for (semidefinite) programs with polynomial inequality constraints, which can be rewritten in terms of sum-of-squares polynomials.","category":"page"},{"location":"#Clustered-Low-Rank-Semidefinite-Programs","page":"Home","title":"Clustered Low-Rank Semidefinite Programs","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A clustered low-rank semidefinite program is defined as","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\n\tmin quad a +  sum_j langle Y^j C^j rangle + langle y brangle \n\ttextst quad  langle Y^j A^j_* rangle + B^T y = c \n\t Y^j succeq 0\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"where the optimization is over the positive semidefinite matrices Y^j and the vector of free variables y. Here langle Y^j A^j_*rangle denotes the vector with entries langle Y^j A^j_prangle and the matrices A^j_p have the structure","category":"page"},{"location":"","page":"Home","title":"Home","text":"\tA_p^j = bigoplus_l=1^L_j sum_l=1^L_j sum_rs=1^R_j(l) A_p^j(lr s) otimes E_rs^R_j(l)","category":"page"},{"location":"","page":"Home","title":"Home","text":"For fixed j l, the matrices A_p^j(lrs) are either normal matrices or of low rank. Furthermore, the full matrices are symmetric, which translates to the condition A_p^j(lr s)^sf T =  A_p^j(ls r). The matrix E_rs^n is the n times n matrix with a one at position (rs) and zeros otherwise. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"One example where this structure shows up is when using polynomial constraints which are converted to semidefinite programming constraints by sampling. Such a semidefinite program with low-rank polynomial constraints is defined as","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\n\tmin quad  a + sum_j langle Y^j C^j rangle + langle y brangle \n\ttextst quad  langle Y^j A^j_*(x) rangle + B^T(x) y = c(x) \n\t Y^j succeq 0\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"where A^j_p(x) have the same structure as before but have now polynomials as entries. This can be obtained from polynomial inequality constraints by sum-of-squares characterizations. The interface currently focusses on such semidefinite programs with polynomial constraints.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The implementations contains data types for a clustered low-rank semidefinite programs (ClusteredLowRankSDP) and semidefinite programs with low-rank polynomial constraints (LowRankPolProblem), where a LowRankPolProblem can be converted into a ClusteredLowRankSDP. The implementation also contains data types for representing low-rank (polynomial) matrices as well as functions and data types for working with samples and sampled polynomials. ","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The solver is written in Julia, and has been registered as a Julia package. Typing using ClusteredLowRankSolver in the REPL will prompt installation if the package has not been installed yet (from Julia 1.7 onwards).","category":"page"},{"location":"#Documentation","page":"Home","title":"Documentation","text":"","category":"section"},{"location":"#Solver","page":"Home","title":"Solver","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"solvesdp","category":"page"},{"location":"#ClusteredLowRankSolver.solvesdp","page":"Home","title":"ClusteredLowRankSolver.solvesdp","text":"solvesdp(sdp; kwargs...)\n\nSolve the clustered SDP with low-rank constraint matrices.\n\nSolve the following sdp:\n\nbeginaligned\n  max  _j=1^J  C^j Y^j  +  b y   \n   A^j_* Y^j + B^j y = c^j\t  j=1ldotsJ \n  Y^j  0 j=1J\nendaligned\n\nwhere we optimize over the free variables y and the PSD block matrices Y^j = diag(Y^j1  Y^jL_j), and A_*^j Y^j denotes the vector with entries A_p^j Y^j. The matrices A^j_p have the same block structure as Y^j. Every A^jl can have several equal-sized blocks A^jl_rs. The smallest blocks have a low rank structure.\n\nKeyword arguments:\n\nprec (default: precision(BigFloat)): the precision used\ngamma (default: 0.9): the step length reduction; a maximum step length of α reduces to a step length of max(gamma*α,1)\nbeta_(in)feasible (default: 0.1 (0.3)): the amount mu is tried to be reduced by in each iteration, for (in)feasible solutions\nomega_p/d (default: 10^10): the starting matrix variable for the primal/dual is omega_p/d*I\nmaxiterations (default: 500): the maximum number of iterations\nduality_gap_threshold (default: 10^-15): how near to optimal the solution needs to be\nprimal/dual_error_threshold (default:10^-30): how feasible the primal/dual solution needs to be\nmax_complementary_gap (default: 10^100): the maximum of <X,Y>/#rows(X) allowed\nneed_primal_feasible/need_dual_feasible (default: false): terminate when the solution is primal/dual feasible\nverbose (default: true): print information after every iteration if true\nstep_length_threshold (default: 10^-7): the minimum step length allowed\ninitial_solutions (default: []): if x,X,y,Y are given, use that instead of omega_p/d * I for the initial solutions\n\n\n\n\n\n","category":"function"},{"location":"#Interface","page":"Home","title":"Interface","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ClusteredLowRankSDP\nLowRankPolProblem\nObjective\nConstraint\nLowRankMatPol\nBlock\napproximatefekete\nCLRSResults","category":"page"},{"location":"#ClusteredLowRankSolver.ClusteredLowRankSDP","page":"Home","title":"ClusteredLowRankSolver.ClusteredLowRankSDP","text":"ClusteredLowRankSDP(sos::LowRankPolProblem; as_free::Vector, prec, verbose])\n\nDefine a ClusteredLowRankSDP based on the LowRankPolProblem sos.\n\nThe PSD variables defined by the keys in the vector as_free will be modelled as extra free variables, with extra constraints to ensure that they are equal to the entries of the PSD variables. Remaining keyword arguments:\n\nprec (default: precision(BigFloat)) - the precision of the result\nverbose (default: false) -  print progress to the standard output\n\n\n\n\n\n","category":"type"},{"location":"#ClusteredLowRankSolver.LowRankPolProblem","page":"Home","title":"ClusteredLowRankSolver.LowRankPolProblem","text":"LowRankPolProblem(maximize, objective, constraints)\n\nCombine the objective and constraints into a low-rank polynomial problem\n\n\n\n\n\n","category":"type"},{"location":"#ClusteredLowRankSolver.Objective","page":"Home","title":"ClusteredLowRankSolver.Objective","text":"Objective(constant, matrixcoeff::Dict{Block, Matrix}, freecoeff::Dict)\n\nThe objective for the LowRankPolProblem\n\n\n\n\n\n","category":"type"},{"location":"#ClusteredLowRankSolver.Constraint","page":"Home","title":"ClusteredLowRankSolver.Constraint","text":"Constraint(constant, matrixcoeff, freecoeff, samples[, scalings])\n\nModels a polynomial constaint of the form\n\n    c(x) = _l  Y^l_rs A^l_rs(x)  + _i y_i B_i(x)\n\nwith samples, where r,s are defined by the Block structure with l as first argument\n\nArguments:\n\nconstant::MPolyElem\nmatrixcoeff::Dict{Block, LowRankMatPol}\nfreecoeff::Dict{Any, MPolyElem}\nsamples::Vector{Vector}\nscalings::Vector\n\n\n\n\n\n","category":"type"},{"location":"#ClusteredLowRankSolver.LowRankMatPol","page":"Home","title":"ClusteredLowRankSolver.LowRankMatPol","text":"LowRankMatPol(eigenvalues::Vector, rightevs::Vector{Vector}[, leftevs::Vector{Vector}])\n\nThe matrix _i λ_i v_i w_i^T.\n\nIf leftevs is not specified, use leftevs = rightevs.\n\n\n\n\n\n","category":"type"},{"location":"#ClusteredLowRankSolver.Block","page":"Home","title":"ClusteredLowRankSolver.Block","text":"Block(l::Any[, r::Int, s::Int])\n\nSpecifies a block corresponding to the positive semidefinite variable l.\n\nSpecifying r,s makes the Block correspond to the r,s subblock of the variable l.\n\n\n\n\n\n","category":"type"},{"location":"#ClusteredLowRankSolver.approximatefekete","page":"Home","title":"ClusteredLowRankSolver.approximatefekete","text":"approximatefekete(basis, samples) -> basis, samples\n\nCompute approximate fekete points based on samples and a corresponding orthogonal basis. The basis consists of sampled polynomials, sampled at samples\n\nThis preserves a degree ordering of basis if present.\n\n\n\n\n\n","category":"function"},{"location":"examples/poly_opt/#Example:-Multivariate-polynomial-optimization-using-symmetry-reduction","page":"Symmetric polynomial optimization","title":"Example: Multivariate polynomial optimization using symmetry reduction","text":"","category":"section"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"In this example, we consider minimizing a multivariate polynomial with S_3 symmetries. The example is inspirated by example 7.1 from Karin Gatermann, Pablo A. Parrilo (2004). See the Examples folder for the file with the code. We consider the polynomial","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"f(xyz) = x^4 + y^4 + z^4 - 4xyz + x + y + z","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"for which we want to find the minimum value f_min = min_xyz f(xyz). Relaxing the problem with a sum-of-squares constraint gives","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"beginaligned\n    max quad M \n    textst quad f - M textis a sum-of-squares\nendaligned","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"where b is a vector of basis polynomials in variables xyz up to a certain degree d. Since the polynomial f is invariant under permuting the variables (i.e., the group action of S_3), it is natural to consider only invariant sum-of-squares polynomials. From Karin Gatermann, Pablo A. Parrilo (2004), we know that any sum-of-squares polynomial invariant under the action of S_3 can be written as","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"    langle Y_1 ww^sf T rangle + langle Y_2 Pi_2 otimes ww^sf T rangle + langle Y_3 Pi_3 otimes ww^sf T rangle","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"where w(xyz) is a vector of basis polynomials of the invariant ring Rxyz^S_3 = Rx+y+z xy+yz+xz xyz, and where Pi_2(xyz) = ((x-y)(y-z)(z-x))^2 and","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"    Pi_3 = beginpmatrix2phi_1^2-6phi_2  -phi_1phi_2+9phi_3  -phi_1phi_2+9phi_3  2phi_2^2- 6phi_1phi_3endpmatrix","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"In particular, Pi_3 is of rank 2 and has the decomposition v_1 v_1^sf T + v_2 v_2^sf T with","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"v_1 = frac1sqrt2beginpmatrix\n    2x-y-z \n    2yz-zx-xy\nendpmatrix quad text and  quad v_2 = sqrtfrac32beginpmatrix\n    y-z \n    zx-xy\nendpmatrix","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"Since we consider sum-of-squares polynomials of a certain degree d, we restrict to the elements of the matrices Pi_i otimes ww^sf T with degree at most lfloor d2 rfloor.","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"To sample this constraint we need a three-variate minimal unisolvent set for invariant polynomials of degree at most d. In this case, one such example are the representatives of the orbits of S_3 of the rational points in the simplex with denominator d, since these points are invariant under S_3 and are minimal unisolvent (see David de Laat, Nando Leijenhorst (2022)). However, if the polynomial space is more complicated it is unclear what a minimal unisolvent set is. To show one approach on this we instead make a grid of points which unisolvent but not minimal, and we use the approach of David de Laat, Nando Leijenhorst (2022) through approximatefekete to choose a good subset of these points and a corresponding good basis for w.","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"As for the example of the Delsarte bound, the objective is simply one free variable M with coefficient 1. We also create a function to generate an invariant basis with variables xyz.","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"using ClusteredLowRankSolver, BasesAndSamples, AbstractAlgebra\n\nfunction invariant_basis(x,y,z, d)\n    # create a vector with a precise type\n    v = [(x*y*z)^0]\n    for deg=1:d, j=0:div(deg,3), i=0:div(deg-3j,2)\n        # monomials in the invariant elementary polynomials\n        # ordered by degree\n        push!(v, (x+y+z)^(deg-2i-3j) * (x*y+y*z+z*x)^i * (x*y*z)^j)\n    end\n    return v\nend\n\nfunction min_f(d)\n\n    obj = Objective(0, Dict(), Dict(:M => 1))","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"In this case, we need a three-variate polynomial ring, and a basis of invariant polynomials. We also use approximatefekete to find a subset of the sample points with a good basis.","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"    FF = RealField\n    R, (x,y,z) = PolynomialRing(FF, [\"x\", \"y\", \"z\"])\n    # The polynomial f:\n    f =  x^4 + y^4 + z^4 - 4x*y*z + x + y + z\n\n    # An invariant basis up to degree d:\n    basis = invariant_basis(x, y, z, 2d)\n    # For the sum-of-squares polynomials we have to\n    # select elements of the basis based on the degree\n    degrees = [total_degree(p) for p in basis]\n\n    # generate samples and a good basis\n    cheb_points = [vcat(sample_points_chebyshev(2d+k)...) for k=0:2]\n    samples_grid = [[cheb_points[1][i+1], cheb_points[2][j+1], cheb_points[3][k+1]]\n        for i=0:2d for j=0:2d+1 for k=0:2d+2]\n    basis, samples = approximatefekete(basis, samples_grid)","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"Now we will construct the constraint matrices corresponding to the sum-of-squares parts. Although approximatefekete returns a polynomial basis only characterized by the evaluations on the sample points, we can work with it as if it were normal polynomials. Among others, this means that we can take the kronecker product of the sampled basis polynomials and a vector of polynomials.","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"    psd_dict = Dict()\n    symmetry_weights = [[[R(1)]],\n            [[R((x-y)*(y-z)*(z-x))]],\n            [[1/sqrt(FF(2))*(2x-y-z),1/sqrt(FF(2))*(2y*z-x*z-x*y)],\n                [sqrt(FF(3)/FF(2))*(y-z),sqrt(FF(3)/FF(2))*(x*z-x*y)]]]\n    for swi=1:length(symmetry_weights)\n        rank = length(symmetry_weights[swi])\n        # create a decomposition of Π_i ⊗ ww^T in terms of polynomial vectors\n        vecs = [kron(symmetry_weights[swi][r], basis) for r=1:rank]\n        # This has in general too many entries,\n        # so we will remove the ones with too high degree.\n        for r=1:rank\n            len = length(basis)\n            # we keep the elements with degree at most d.\n            keep_idx = [i for i=1:length(vecs[r])\n                if total_degree(symmetry_weights[swi][r][div(i-1,len)+1]) +\n                    degrees[(i-1)%len+1] <= d]\n            vecs[r] = vecs[r][keep_idx]\n        end\n        psd_dict[Block((:trivariatesos,swi))] =\n                LowRankMatPol([R(1) for r=1:rank], vecs)\n    end","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"Now we can formulate the constraint and solve the ClusteredLowRankSDP:","category":"page"},{"location":"examples/poly_opt/","page":"Symmetric polynomial optimization","title":"Symmetric polynomial optimization","text":"    # the constraint is SOS + M = f\n    constr = Constraint(f, psd_dict, Dict(:M => R(1)), samples)\n    pol_problem = LowRankPolProblem(true, obj, [constr])\n    sdp = ClusteredLowRankSDP(pol_problem)\n\n    solvesdp(sdp)\nend","category":"page"},{"location":"examples/delsarte/#Example:-the-Delsarte-bound","page":"Delsarte LP bound","title":"Example: the Delsarte bound","text":"","category":"section"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"In this example we show how the Delsarte linear programming bound P. Delsarte, J. M. Goethals, J. J. Seidel (1977) for the spherical code problem can be modeled and solved using ClusteredLowRankSolver. See the Examples folder for the file with the code. Let P_k^n be the Gegenbauer polynomial of degree k with parameter n2-1, normalized such that P_k^n(1) = 1. The Delsarte bound for dimension n, degree 2d, and angle theta can be written as","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"beginaligned\n    min quad  M  \n    textst quadsum_k=1^2d a_k P_k^n(x) leq -1  quad x in -1cos(theta)\n      sum_k=1^2d a_k  -  M leq -1 \n     a_k geq 0\nendaligned","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"This gives an upper bound on the size of a spherical code on S^n-1 with x cdot yleqcos(theta) for any distinct x and y. Note that we could remove the free variable M by using the objective 1+sum_k a_k, but for illustration purposes we include this free variable in the formulation.","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"To model this as a clustered low-rank semidefinite program, we equate the polynomial","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"- 1 - sum_k a_k P^n_k(x)","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"to the weighted sums-of-squares polynomial","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"langle Y_1b(x)b(x)^T rangle + (x+1)(cos(theta)-x) langle Y_2b(x)b(x)^Trangle","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"for a vector of basis polynomials b(x). By a theorem of Lukács, one can always write the polynomial in this form. Moreover, we can truncate the vectors b(x) such that both terms have degree at most 2d. Then we sample the polynomials on 2d+1 points x_1ldotsx_2d+1 in -1cos(theta) to obtain semidefinite constraints (any set of 2d+1 distinct points is minimal unisolvent for the univariate polynomials up to degree 2d).","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"Furthermore, to model the linear inequality constraint, we introduce a slack variable s geq 0 such that we need sum_k=1^2d a_k + s - M = -1. Together this gives the semidefinite program","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"beginaligned\n    min quad  M  \n    textst sum_k=1^2d a_k P_k^n(x_l) + langle b_d(x_l)b_d(x_l)^T Y_1 rangle \n    quad + langle g(x_l)b_d-1(x_l)b_d-1(x_l)^T Y_2 rangle = -1quad l=1 ldots 2d+1 \n      sum_k=1^2d a_k + s - M  =-1 quad \n     a_k geq 0\n     s geq 0 \n     Y_i succeq 0 i=12\nendaligned","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"where b_d is a basis of polynomials up to degree d and","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"g(x) = (x+1)(cos(theta)-x)","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"Now that we formulated the semidefinite program, we can model it in Julia. We use the PolynomialRing structure of AbstractAlgebra to define the polynomials. For the bases and samples, we use the auxilliary package BasesAndSamples. And of course, we also need ClusteredLowRankSolver. We will start with the objective and constraints after which we define the complete program.","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"The objective is simple, since we only optimize the value of the free variable M. This gives the start of our script:","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"using ClusteredLowRankSolver, BasesAndSamples, AbstractAlgebra\n\nfunction delsarte(n=3, d=10, costheta=1//2)\n\n    # Initialize the objective with additive constant 0,\n    # no dependence on matrix variables,\n    # and dependence with coefficient 1 on the free variable :M\n    obj = Objective(0, Dict(), Dict(:M => 1))\n","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"For the first constraint, we need the Gegenbauer polynomials, which are  defined in the package BasesAndSamples. In this constraint we only use the positive semidefinite matrix variables, for which we collect the constraint matrices in a Dict:","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"    R, (x,) = PolynomialRing(RealField, [\"x\"])\n    # A vector consisting of the Gegenbauer polynomials\n    # of degree at most 2d\n    gp = basis_gegenbauer(2d, n, x)\n    psd_dict1 = Dict()\n    for k = 1:2d\n        # The eigenvalues are the gegenbauer polynomials,\n        # and the vectors have the polynomial 1 as entry:\n        psd_dict1[Block((:a, k))] = hcat([gp[k+1]])\n    end","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"Since Julia 1.7, it is possible to construct matrices with gpk+1 instead of hcat. Note that here we use a general (high-rank) constraint matrix for the variables a_k. For the sums-of-squares part, we define a basis and the set of samples points. We use approximatefekete to obtain a basis which is orthogonal on the sample points.","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"    # 2d+1 Chebyshev points in the interval [-1, cos(θ)]\n    samples = sample_points_chebyshev(2d, -1, costheta)\n    # A basis for the Chebyshev polynomials of degree at most 2d\n    basis = basis_chebyshev(2d, x)\n    # Optimize the basis with respect to the samples\n    sosbasis, samples = approximatefekete(basis, samples)","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"The samples need to be a vector of numbers, because we also can consider multivariate polynomials. Since the second sum-of-squares has a weight of degree 2, we use a basis up to degree d-1 in that case.","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"    psd_dict1[Block((:SOS, 1))] =\n        LowRankMatPol([1], [sosbasis[1:d+1]])\n    psd_dict1[Block((:SOS, 2))] =\n        LowRankMatPol([(1+x)*(costheta-x)], [sosbasis[1:d]])\n    constr1 = Constraint(-1, psd_dict1, Dict(), samples)","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"In the last line, we define the constraint (with constant polynomial -1). The extra dictionary corresponds to the coefficients for the free variables, which we do not use in this constraint.","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"The second constraint seems simpler since it does not involve polynomials. However, since we first define a LowRankPolProblem, we need to write it as a constraint with polynomials of degree 0. We also introduce a slack variable, this gives the constraint sum_k a_k + s - M = -1, where s geq 0.","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"    # The free variable M has coefficient -1:\n    free_dict2 = Dict(:M => -1)\n    # The variables a_k and the slack variable have coefficient 1:\n    psd_dict2 = Dict()\n    for k = 1:2d\n        psd_dict2[Block((:a, k))] = hcat([1])\n    end\n    psd_dict2[Block(:slack)] = LowRankMatPol([1], [[1]])\n    # We have one sample, which is arbitrary\n    constr2 = Constraint(-1, psd_dict2, free_dict2, [[0]])","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"Now we can define the LowRankPolProblem and convert it to a ClusteredLowRankSDP. When converting, we can choose to model some of the PSD variables as free variables, which can be beneficial if the corresponding constraint matrices are of relatively high rank or if they couple a lot of polynomial constraints. In this case, we just illustrate the option by modelling a_0 as free variable. This adds the constraint X = a_0 with a positive semidefinite variable X (in this case a 1 times 1 matrix), and replaces a_0 by a free variable in every constraint.","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"    # The first argument is false because we minimize:\n    sos = LowRankPolProblem(false, obj, [constr1, constr2])\n    sdp = ClusteredLowRankSDP(sos; as_free = [(:a, 0)])","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"Now we can solve the semidefinite program with","category":"page"},{"location":"examples/delsarte/","page":"Delsarte LP bound","title":"Delsarte LP bound","text":"    status, result, time, errorcode = solvesdp(sdp)\nend","category":"page"}]
}
